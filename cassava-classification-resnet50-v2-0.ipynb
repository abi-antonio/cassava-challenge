{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis notebook will will train an image classifier for the Cassava Leaf Disease Classification Kaggle Challenge"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import re\nimport numpy as np\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nfrom keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###### Seed Everything\nSEED = 42\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GCS_DS_PATH = KaggleDatasets().get_gcs_path('cassava-leaf-disease-classification')\nGCS_DS_PATH = '/kaggle/input/cassava-leaf-disease-classification/'\n\nFILEPATHS =  tf.io.gfile.glob(GCS_DS_PATH + '/train_tfrecords/*.tfrec')\n#FILEPATHS = tf.io.gfile.glob(GCS_DS_PATH + 'train_tfrecords/ld_train00-1338.tfrec')\n#FILEPATHS = [GCS_DS_PATH + 'train_tfrecords/ld_train00-1338.tfrec']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\n# Constants\nNUM_TRAINING_IMAGES = int(count_data_items(FILEPATHS) )\nprint(\"Number of training images:\", NUM_TRAINING_IMAGES)\n\n\n#IMAGE_SIZE = [800,600] # Original image size\nDIM = 128\nIMAGE_SIZE = [DIM, DIM] # we can't use the original image size because it's too big, and we'll encounter an Out of memory error\nCLASSES = 5\n\nBATCH_SIZE = 64\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\nEPOCHS = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(image_data):\n    \"\"\"\n    This function is used to read the image column of the TFRecord\n    \n    Input\n    image_data - Tensor\n    \"\"\"\n    \n    # Read the image as a numpy array\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    \n    # Originally, the image is saved as a string so we need to convert the it into floats.\n    # We will also normalize the values so that it lies in the [0,1] range\n    image = tf.cast(image, tf.float32) / 255.0\n    \n    # Resize the image into our desired dimension\n    image = tf.image.resize(image, [DIM, DIM])\n    #image = tf.image.resize(image, [IMAGE_SIZE[0], IMAGE_SIZE[1]]) # Use original image size\n    \n    # Reshape the image into (length, width, color_channel)\n    image = tf.reshape(image, [DIM, DIM, 3])\n    #image = tf.reshape(image, [IMAGE_SIZE[0], IMAGE_SIZE[1], 3]) # Use original image size\n    \n    return image\n\ndef read_labeled_tfrecord(example):\n    \"\"\"\n    This function is used to read one example(aka one TFRecord) from a TFRecordDataset\n    \n    Input\n    example - TFRecordDataset\n    \"\"\"\n    \n    # To read one record, we need to define what are the expected \"columns\" of the data\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        \"target\": tf.io.FixedLenFeature([], tf.int64), \n    }\n    \n    # Read one example\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    \n    # Get the image column. Since the image column is saved as a string, we need to convert it into a matrix\n    image = decode_image(example['image'])\n    \n    # Get the label column\n    label = tf.cast(example['target'], tf.int32)\n    \n    return image, label \n\ndef load_dataset(filenames, labeled = True, ordered = False):\n    \"\"\"\n    This function will create a TFRecordDataset from the given filenames\n    \n    Input\n    filenames - list of strings\n    \"\"\"\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False \n        \n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.with_options(ignore_order) \n    dataset = dataset.map(read_labeled_tfrecord ) \n    return dataset\n\ndef get_training_dataset(dataset):\n    \"\"\"\n    This function will retrieve a subset/batch of records from the given dataset\n    \n    Input\n    dataset - TFRecordDataset\n    \"\"\"\n    dataset = dataset.repeat() \n    dataset = dataset.map(onehot)\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    return dataset\n\ndef onehot(image,label):\n    \"\"\"\n    This function converts the label of an image to one-hot encoding\n    \"\"\"\n    \n    return image,tf.one_hot(label,CLASSES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the dataset\ntrain_dataset = load_dataset(FILEPATHS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## define and compile the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ResNet50 base model\nrnet = tf.keras.applications.ResNet50(\n        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n        weights='imagenet',\n        include_top=False\n    )\n# freeze the base model\nrnet.trainable = False\n\n# Create a new model\nmodel = tf.keras.Sequential([\n    rnet\n    , tf.keras.layers.GlobalAveragePooling2D()\n    , tf.keras.layers.Dropout(0.2)\n    , tf.keras.layers.Dense(5, activation='softmax', dtype='float32')\n])\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n              metrics=['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MY_MODEL_FILE = '/kaggle/input/my-cassava-data/resnet50_model_v9.h5' # v5-9 model takes images of size (128, 128, 3)\n\nmodel = load_model(MY_MODEL_FILE, custom_objects = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.layers[-1].get_weights()\n\n# Update Dropout Rate\nmodel.layers[-1].rate = 0.5\n#model = tf.keras.models.clone_model(model)\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n              metrics=['categorical_accuracy'])\n#model.load_weights('weights_resnet50_model_v9.h5')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Freeze the first layer (base ResNet50 model)\nmodel.layers[0].trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the topmost layer\nhistory = model.fit(\n    get_training_dataset(train_dataset),\n    steps_per_epoch = STEPS_PER_EPOCH,\n    epochs = 5,\n    #epochs = EPOCHS,\n    verbose=1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fine tune all the layers (base restnet50 and top layer)\nrnet.trainable = True\nmodel.summary()\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n              metrics=['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    get_training_dataset(train_dataset),\n    steps_per_epoch = STEPS_PER_EPOCH,\n    epochs = 10,\n    verbose=1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save model\nmodel_filename = 'resnet50_model_v10.h5'\nprint(\"save model to\", model_filename)\nmodel.save(model_filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save model's weights only\nmodel_weights_filename = 'weights_' + model_filename\nprint(\"save model's weights to\", model_weights_filename)\nmodel.save_weights(model_weights_filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"FILENAMES =  tf.io.gfile.glob('/kaggle/input/cassava-leaf-disease-classification/train_tfrecords/*.tfrec')\ntrain_dataset = load_dataset(FILENAMES)\n\ndef get_test_dataset(dataset):\n    \"\"\"\n    This function will retrieve a subset/batch of records from the given dataset\n    \n    Input\n    dataset - TFRecordDataset\n    \"\"\"\n    dataset = dataset.map(onehot)\n    #dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    return dataset\n\nresult = model.evaluate(get_test_dataset(train_dataset))\ndict(zip(model.metrics_names, result))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}